{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo\n",
    "Welcome to the object detection task walkthrough!  This notebook will walk you step by step through the process of training a fast-rcnn model for object detection, evaluating it during the training, and using a well-trained model to detect objects in some images. The object detection demo uses the Tensorflow Object Detection API, which is an open source framework built on top of TensorFlow that makes it easy to construct, train and deploy object detection models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset\n",
    "Training requires datasets, and object detection model training requires a lot of images with labeled objects. There are some famous datasets used for computer vision: \n",
    "\n",
    "Dataset | Training Set Size | Testing Set Size | Number of Classes | Comments\n",
    ":------:|:---------------:|:---------------------:|:-----------:|:-----------:\n",
    "Flowers|2500 | 2500 | 5 | Various sizes (source: Flickr)\n",
    "[Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html) | 60k| 10k | 10 |32x32 color\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/)| 60k | 10k | 10 | 28x28 gray\n",
    "[ImageNet](http://www.image-net.org/challenges/LSVRC/2012/)|1.2M| 50k | 1000 | Various sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets used for object detection are generally composed of raw images, bounding boxes and classifications. Here, we are using the Oxford-IIIT Pets dataset. Here's a sample from the dataset:\n",
    "<img src=\"https://image.ibb.co/gOGXmz/oxford_pet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw dataset for Oxford-IIIT Pets lives\n",
    "[here](http://www.robots.ox.ac.uk/~vgg/data/pets/). Both the image dataset [`images.tar.gz`](http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz)\n",
    "and the groundtruth data [`annotations.tar.gz`](http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz) are needed. They have been downloaded and unzipped in the path `./data`. \n",
    "\n",
    "For now, the file structure should appear as follows:\n",
    "```lang-none\n",
    "+ ${YOUR_PATH}/\n",
    "    + data/\n",
    "      + images/\n",
    "      + annotations/\n",
    "      - pet_label_map.pbtxt\n",
    "    + models/  \n",
    "    + config/  \n",
    "    + object_detection/\n",
    "    + test_images/\n",
    "    - object_detection_demo.ipynb\n",
    "    - evaluate.ipynb\n",
    "    - model_inference.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensorflow Object Detection API expects data to be in the TFRecord format, a general format used for Tensorflow. Except for the raw data, we need a label map mapping class name to ids. It is constructed according to the format of Google Protocol Buffer(protobuf) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the `create_pet_tf_record` script to convert from the raw Oxford-IIIT Pet dataset into TFRecords, you have to fill in the data directory and the output direcotry for your TFRecord files, we recommend you to put the TFRecord files in the `./data` path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./object_detection/dataset_tools/create_pet_tf_record.py \\\n",
    "    --label_map_path=\"LABEL_MAP_FILE\" \\\n",
    "    --data_dir=\"DATA_DIRECTORY\" \\\n",
    "    --output_dir=\"OUTPUT_DIRECTORY\"\n",
    "# Note: It is normal to see some warnings when running this script. You may ignore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is partitioned into two TFRecord files named `pet_train.record` with 70% images and `pet_val.record` with 30% images, which should be generated in the output directory you set. `pet_train.record` is used for training your model and `pet_val.record` is for evaluating the performance of your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Tensorflow Object Detection API uses protobuf files to configure the\n",
    "training and evaluation process. The schema for the training pipeline can be\n",
    "found in `./object_detection/protos/pipeline.proto`. At a high level, the config\n",
    "file is split into 5 parts:\n",
    "\n",
    "1. The `model` configuration. This defines what type of model will be trained (ie. meta-architecture, feature extractor).\n",
    "2. The `train_config`, which decides what parameters should be used to train model parameters (ie. the training speed, input preprocessing).\n",
    "3. The `eval_config`, which determines what set of metrics will be reported for evaluation.\n",
    "4. The `train_input_config`, which defines what dataset the model should be trained on.\n",
    "5. The `eval_input_config`, which defines what dataset the model will be evaluated on. Typically this should be different than the training input dataset.\n",
    "\n",
    "Sample model configurations have been provided\n",
    "in the `./object_detection/samples/configs` folder. Different training models need different configurations. Here we use the faster RCNN model with inception v2 neural network for training, whose configuration file is `faster_rcnn_inception_v2_pets.config`. You can copy it into the `./config` directory and check the parameters in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking Model Parameters\n",
    "\n",
    "The model parameters in `model` part in sample configuration has been set properly for training. The only parameter you should change is the `num_classes` field should be changed to a value suited for the dataset the user is training on. You can check the label map file for the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs\n",
    "\n",
    "The Tensorflow Object Detection API accepts inputs in the TFRecord file format.\n",
    "Users must specify the locations of both the training and evaluation files.\n",
    "Additionally, users should also specify a label map, which define the mapping\n",
    "between a class id and class name. The label map should be identical between\n",
    "training and evaluation datasets.\n",
    "\n",
    "An example input configuration looks as follows:\n",
    "\n",
    "```\n",
    "tf_record_input_reader {\n",
    "  input_path: \"/usr/home/username/data/train.record\"\n",
    "}\n",
    "label_map_path: \"/usr/home/username/data/label_map.pbtxt\"\n",
    "```\n",
    "Change the `tf_record_input_reader` parameters in both `train_input_config` and `eval_input_config` parts. Note that the label map file `label_map_path` is generally the same for the training and evaluation processes.\n",
    "\n",
    "## Configuring the Trainer\n",
    "\n",
    "The `train_config` defines parts of the training process:\n",
    "\n",
    "1. Model parameter initialization.\n",
    "2. Input preprocessing.\n",
    "3. SGD parameters.\n",
    "\n",
    "### Model Parameter Initialization\n",
    "\n",
    "While optional, it is highly recommended that users utilize other object\n",
    "detection checkpoints. Training an object detector from scratch can take days.\n",
    "To speed up the training process, it is recommended that users re-use the\n",
    "feature extractor parameters from a pre-existing object classification or\n",
    "detection checkpoint. `fine_tune_checkpoint` provide a path to\n",
    "the pre-existing checkpoint. Here we have downloaded a faster RCNN model with inception v2 neural network pre-trained with COCO dataset. You can find it in `./models/`\n",
    "\n",
    "Change the `fine_tune_checkpoint` parameter to the downloaded checkpoint file. You can find many pre-trained models in [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\n",
    "\n",
    "### Input Preprocessing\n",
    "\n",
    "The `data_augmentation_options` in `train_config` can be used to specify\n",
    "how training data can be transformed to augment the dataset.\n",
    "\n",
    "### SGD Parameters\n",
    "\n",
    "The remaining parameters in `train_config` are hyperparameters for gradient\n",
    "descent, representing the training speed. Here we change the `num_steps` to xxx to limit the training steps. If not set, the training job will\n",
    "run indefinitely until the user kills it. \n",
    "\n",
    "## Configuring the Evaluator\n",
    "\n",
    "Currently evaluation is fixed to generating metrics as defined by the PASCAL VOC\n",
    "challenge. The parameters for `eval_config` are set to reasonable defaults and\n",
    "typically do not need to be configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "After filling the configuration file, let's create two directories for saving the checkpoints of training model and the results of evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir  -p ./models/faster_rcnn_inception_v2_pet_2018_10_08/train\n",
    "!mkdir  -p ./models/faster_rcnn_inception_v2_pet_2018_10_08/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the file structure should appear as follows:\n",
    "```\n",
    "+data\n",
    "  - label_map file\n",
    "  - train TFRecord file\n",
    "  - eval TFRecord file\n",
    "+models\n",
    "  + faster_rcnn_inception_v2_coco_2018_01_28\n",
    "  + faster_rcnn_inception_v2_pet_2018_10_08\n",
    "    +train\n",
    "    +eval\n",
    "+config\n",
    "  - pipeline config file\n",
    "```\n",
    "and now we can start the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./object_detection/train.py \\\n",
    "    --logtostderr \\\n",
    "    --pipeline_config_path=\"PATH_TO_YOUR_PIPELINE_CONFIG\" \\\n",
    "    --train_dir=\"PATH_TO_TRAIN_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `${PATH_TO_YOUR_PIPELINE_CONFIG}` points to the pipeline config and\n",
    "`${PATH_TO_TRAIN_DIR}` points to the directory in which training checkpoints\n",
    "and events will be written to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate!\n",
    "\n",
    "Evaluation is run as a separate job. The eval job will periodically poll the\n",
    "train directory for new checkpoints and evaluate them on a test dataset. So it should run synchronously with the training job. Execute the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# From the tensorflow/models/research/ directory\n",
    "python ./object_detection/eval.py \\\n",
    "    --logtostderr \\\n",
    "    --pipeline_config_path=\"PATH_TO_YOUR_PIPELINE_CONFIG\" \\\n",
    "    --checkpoint_dir=\"PATH_TO_TRAIN_DIR\" \\\n",
    "    --eval_dir=\"PATH_TO_EVAL_DIR\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `${PATH_TO_YOUR_PIPELINE_CONFIG}` points to the pipeline config,\n",
    "`${PATH_TO_TRAIN_DIR}` points to the directory in which training checkpoints\n",
    "were saved (same as the training job) and `${PATH_TO_EVAL_DIR}` points to the\n",
    "directory in which evaluation events will be saved. Because one notebook has only one thread, you can run the command in shell, or in another notebook [here](evaluate.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Results with TensorBoard\n",
    "\n",
    "Progress for training and eval jobs can be inspected using Tensorboard. If\n",
    "using the recommended directory structure, Tensorboard can be run using the\n",
    "following command:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=\"PATH_TO_MODEL_DIRECTORY\"\n",
    "```\n",
    "\n",
    "where `${PATH_TO_MODEL_DIRECTORY}` points to the directory that contains the\n",
    "train and eval directories. Please note it may take Tensorboard a couple minutes\n",
    "to populate with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting a trained model for inference\n",
    "\n",
    "After your model has been trained, you should export it to a Tensorflow\n",
    "graph proto. A checkpoint will typically consist of three files:\n",
    "```\n",
    "* model.ckpt-${CHECKPOINT_NUMBER}.data-00000-of-00001,\n",
    "* model.ckpt-${CHECKPOINT_NUMBER}.index\n",
    "* model.ckpt-${CHECKPOINT_NUMBER}.meta\n",
    "```\n",
    "After you've identified a candidate checkpoint to export, run the following\n",
    "command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./object_detection/export_inference_graph.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --pipeline_config_path=\"PATH_TO_YOUR_PIPELINE_CONFIG\" \\\n",
    "    --trained_checkpoint_prefix=\"TRAIN_CHECKPOINT_PREFIX\" \\\n",
    "    --output_directory=\"OUTPUT_DIRECTORY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, you should see a graph named output_inference_graph.pb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
